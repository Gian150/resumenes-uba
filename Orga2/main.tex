\input{../document.setup}

\addbibresource{bibliography.bib}

\title{Organización del Computador II}
\author{Gianfranco Zamboni}

\input{../page.setup}

\newcommand{\red}[1]{{\color{red}#1}}  			% Rojo, duh (?)

\begin{document}
	\maketitle
	\tableofcontents

\newpage
\section{Introducción}

La \textbf{arquitectura de una computadora} es el conjunto de recursos accesibles para el programador que, por lo general, se mantienen a lo largo de los diferentes modelos de procesadores de esa arquitectura (puede evolucionar pero la tendencia es mantener compatibilidad hacia los modelos anteriores).

Diseñar una arquitectura implica diseñar las interfaces hardware/software para crear un sistema computacional que posea los requerimientos funcionales, de perfomance, consumo (de energía) y de costo (económico) adecuados para realizar determinadas tareas.

\begin{wrapfigure}{r}{0.5\textwidth}
	\centering
	\includegraphics[width=0.25\textwidth]{imagenes/arquitectura}
	\caption{La computadora definida en niveles de abstracción}
	\label{fig:intro::arquitectura}
\end{wrapfigure}

Estas tareas son problemas que pasaron por varias transformaciones (desde su descripción en un lenguaje natural hasta convertirse en un programa) y deben ser ejecutadas por una computadora. La tarea del arquitecto consiste en diseñar el \textbf{Instruction Set Architecture} (ISA), un conjunto de instrucciones que usarán los programas compilados para decir al micropocesador que hacer. El ISA es implementado por un conjunto de estructuras de hardware conocidas como \textbf{microarquitectura}.

El ISA y la microarquitectura sientan las bases para que el diseño del procesador que conseguir el balance adecuado de los factores mencionados para llevar a cabo ciertas tareas de la manera más optima posible. Habrá casos en los que daremos prioridad a un subconjunto de ellos en detrimento de otros (por ejemplo, podriamos elegir mejorar perfomance y aumentar el costo, o quitar performance para mejorar el consumo energético).

\subsection{Componentes del ISA}
El ISA es la especificación completa de la interfaz entre los programas y el hardware que debe llevar a cabo las operaciones. Entre otras cosas, especifica:

\subsubsection{Regitros}
\begin{itemize}
	\item \textbf{Registros:} Celdas de memoria dentro del cpu que son usados para almacenar los datos necesarios para ejecutar una instrucción. Éstos son visibles al programa y se clasifican según su uso: Acumuladores, De dirección ó De Proposito General.
	\item  \textbf{Instrucciones}: Tareas que pueden ser llevadas a cabo por la computadora. Cada una de ellas está compuesta por su \textbf{opcode} (que se espera que la computadora haga) y sus \textbf{operandos} (a que datos debe hacerlo). En una ISA, podremos encontrar tres tipos de instrucciones:
	\begin{itemize}
		\item De \textbf{Operacion}: Procesan datos.
		\item De \textbf{trasnporte}: Transportan información entre la memoria, los registros y los dispositivos de entrada salida.
		\item De \textbf{control (Branching)}: Modifican la secuencia de instrucciones a ser ejecutada, es decir, permiten ejecutar instrucciones que no están almacenadas secuencialmente.
	\end{itemize}
	
	Dependiendo que valores puedan modificar las instrucciones de operación, podremos clasificar las arquitecturas en: \textbf{Arquitecturas Load/Store} (solo pueden operar en registros) o \textbf{Arquitecturas memory/memory} (se pueden modificar los valores directamente en memoria)
	\item \textbf{Tipos de datos}: Representación deben tener ciertos valores para que puedan ser interpretados por la microarquitectura.
	\item \textbf{Espacio de memoria}: La cantidad de bloques univocamente distinguibles en memoria y el tamaño de cada uno de estos bloques
	\item \textbf{Direccionamiento}: Los mecanismos usados por la computadora para saber donde están almacenados los datos. Puede ser:
	\begin{itemize}
		\item \textbf{Inmediato:} El operando está incluido en la instrucción.
		\item \textbf{Directo o absoluto:} El operando es la dirección de memoria donde se encuentra el valor a ser utilizado.
		\item \textbf{Indirecto:} El operando es una dirección de memoria donde se encuentra la dirección de memoria en la que está almacenado el valor deseado.
		\item \textbf{De desplazamiento:} La instrucción toma como operandos una dirección de memoria que se toma como \textbf{base} y un \textbf{offset}, que es un número que indica cuanto hay que desplazar la base para encontrar el valor deseado, es decir $dir = base + offset$
		\item \textbf{Indexado:} Lo mismo que el anterior, pero con el \textit{offset} guardado en un registro.
		\item \textbf{De Memoria Indirecta:} El operando es un registro en el que se encuentra guardada la dirección de memoria indirecta.
	\end{itemize}
	\item \textbf{I/O Interface}: Como comunicarse con los dispositivos de entrada/salida. Puede ser por medio de instrucciones especiales o mapeos de ciertas regiones memoria para uso de esos dispositivos.
	\item \textbf{Modos de privilegio:} Quien puede y quien no puede ejecutar ciertas instrucciones 
	\item \textbf{Manejo de expcepciones e interrupciones:} Qué debe suceder si una instrucción falla o cuando un dispositivo necesita usar el microprocesador.
	\item \textbf{Soporte de memoria virtual:} Si soporta o no el uso de \textbf{memoria virtual}, es decir, si cada programa tiene la ilusión de estar un espacio de memoria secuencial cuando en realidad el sistema operativo realiza el manejo de la memoria principal.
\end{itemize}

\subsubsection{Arquitectura de Von Neumann}\label{sec::Intro::ISA::Von_Neuman}
Como se vió en Organización del computador I, las mayoría de las ISA usadas actualmente usan el modelo de Von Neumann. Éste es un ciclo de cuatro etapas: 

\begin{enumerate}
	\item \textbf{Fecth:} Se utiliza un \textbf{program counter} para saber donde está almacenada la proxima instrucción a ser ejecutada. Y se carga desde la memoria.
	\item \textbf{Decode:} Se decodifica la instrucción fetcheada y se consiguen los operandos (literales y registros) correspondientes.
	\item \textbf{Execute:} En esta etapa se busca en memoria los datos requeridos (si es necesario) y se procesa los datos acorde a la instrucción.
	\item \textbf{Write Back:} Se almacenan los resultados obtenidos en el lugar indicado.
\end{enumerate}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.4\textwidth]{imagenes/von-neuman-arquichtecture}
	\caption{Arquitectura de Von Newmman}
	\label{fig:intro:componentesIsa::vonsneumanarquichtecture}
\end{figure}
Cada instrucción es extraída de la memoria usando la dirección indicada por el \textbf{Instruction Pointer}. La unidad de control se encarga de indicar a la memoria si son necesarios otros valores para poder llevar a cabo su ejecución y luego pasa todo los datos a la unidad de procesamiento.

Si bien este es el ciclo que ``ve'' un programador, la implementación de las ISA (microarquitectura) usa estructuras de hardware más complejas que permiten acelerar la ejecución de cada fase.

\subsection{Microarquitectura}
La micoarquitectura es la implementación a nivel hardware de la ISA, es decir, es un conjunto de componentes electrónicos organizados de cierta manera para que respeten esas especificaciones. En la sección \ref{sec::Intro::ISA::Von_Neuman}, vimos como el usuario ve el ciclo de instrucciones. Desde el punto de vista de la implementación (hardware), el ciclo es realizado por unidades de procesamiento que operan sobre los datos de acuerdo a ciertas señales. 

Cada instrucción es una señal que usa el procesador de instrucciones para decidir que conjunto de componentes electrónicos deben ser activados para poder llevar a cabo la operación deseada. Especificamente, las instrucciones indican: 

\begin{itemize}
	\item \textbf{Datapath:} Que elementos deben manejar y transformar los datos (unidades de procesamiento, de almacenamiento y estructuras de hardware que permiten el flujo de datos)
	\item \textbf{Control Logic:} Que elementos de hardware determinan las señales de control que indican al datapath lo que debe hacer con los datos.
\end{itemize}

En otras palabras, la microarquitectura comprende la tecnología utilizada para construir el hardware, la organización e interconexión de la memoria, el diseño de los bloques de CPU y la implementación de distintos mecanismos de procesamiento que no son visibles para el programador. Algunas de las características encontradas en las microarquitecturas actuales son:

\begin{itemize}
	\item Pipelining
	\item Ejecución de múltiple instrucciones simultáneamente.
	\item Ejecución fuera de orden
	\item y Cachés de dato e instrucciones separadas, entre otros.
\end{itemize}

%\subsection{Medidas de Performance}
%La escala y complejidad de los sistemas de software modernos, junto con las técnicas usadas por los diseñadores de hardware para mejorar el rendimiento de los dispositivos, ha logrado hacer que el rendimiento pueda depender de varios factores.
%
%\subsubsection{Response time}
%A veces, mediremos el rendimiento de una computadora en base a su tiempo de respuesta (\textbf{response time} o \textbf{execution time}) - el tiempo entre que pasa entre que una tarea empieza y termina -. Este tiempo se mide en segundos por programa y mide el tiempo total que toma completar una tarea, incluyendo accesos a memoria, operaciones del sistema, etc.
%
%En la práctica, la computadora es compartida por varios programas y los microprocesadores deben ejecutar varias tareas simultáneamente. En estos casos, deberemos tener en cuenta que la tarea que la tarea que se está evaluando, no está siendo ejecutada todo el tiempo por lo qué habrá que distinguir el tiempo que el procesador pasa ejecutando la tarea  \textbf{CPU Time} del tiempo en el que está procesando otros programas.
%
%Entonces, el tiempo de ejecución se define de la siguiente manera:
%
%$$Response~Time = \frac{Clock~Cycles~spent~on~task}{Clock~Rate}$$
%
%Donde \textit{Clock Cycle} son la cantidad de ciclos de reloj (ticks) que se utilizaron en la tarea y \textit{Clock Rate} es el tiempo que dura cada tick del reloj.

\newpage
\part{Instruction Level Parallelism}
\section{Pipelining}
El Pipeline es una técnica que permite superponer el procesamiento de múltiples instrucciones en una ejecución. Una vez que la primera instrucción fue fetcheada y pasa a la etapa de decodificación, se comienza a fetchear la siguiente instrucción. 

Bajo condiciones ideales y con un gran número de instrucciones, la mejora en velocidad de ejecución es directamente proporcional a la cantidad de etapas en el pipe. Es decir, un pipeline de 5 etapas, es aproximadamente 5 veces más rápido que el procesamiento secuencial.  

Si bien, el pipelining, reduce el tiempo de ejecución de un programa, debemos notar que no lo hace modificando el tiempo que se tarda en procesar una instrucción (\textbf{latency / latencia}), sino que aumenta la cantidad de instrucciones que se procesan por unidad de tiempo (\textbf{throughput}).
\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\linewidth]{imagenes/pipelining}
	\caption{Analogía de la lavandería: 4 personas tienen que lavar, secar, doblar y guardar su ropa sucia. Solo disponemos de una lavadora, una secadora, un ``doblador'' y un ropero. Si cada parte del proceso toma 30 minutos, y lo realizamos de manera secuencial, entonces completarlo para las 4 personas tomaría ocho horas, mientras que si lo hacemos con un pipeline el tiempo se reduce a tres horas y media.}
	\label{fig:pipelining}
\end{figure}

Para facilitar una implementación efectiva del pipeline, el set de instrucciones propuesto debe permitir que todas las etapas tarden el mismo tiempo en ser ejecutadas. Esto implica que hay que tener ciertas consideraciones al momento de diseñarlo:

\begin{itemize}
	\item Todas las instrucciones deben tener la misma longitud (en la arquitectura IA-32, donde las instrucciones tienen longitud variada, esto se logra descomponiendo cada instrucción en micro-operaciones de longitud fija y a estas microoperaciones se les aplica el pipelining).
	\item Todas las instrucciones deben tener la misma estructura, es decir, la cantidad de parámetros y la ubicación dentro de la instrucción que los explicita deben ser las mismas (o lo más parecidas posibles). 
	\item Los operandos deben estar alineados en memoria, esto es, las direcciones de memoria que ocupan deben ser múltiplo del tamaño de las palabras usadas para que los datos puedan ser transferidos en una única etapa del pipeline.
\end{itemize}

\subsection{Pipeline Hazards}
Hay situaciones, en las que una instrucción no puede ser ejecutada en el siguiente ciclo de reloj debido a algún obstaculo (\textbf{hazards}). Estos eventos pueden llegar a detener el flujo del pipeline (\textbf{pipeline stall}) y generar una demora en el procesamiento de las instrucciones. A continuación veremos los tres tipos de obstaculos que se pueden dar:

\begin{itemize}
	\item \textbf{Estructurales (Structural Hazards):} El hardware no soporta la combinación de instrucciones que queremos ejecutar en el mismo ciclo de reloj. Por ejemplo, si una instrucción debe acceder a memoria mientras otra debe realizar un fetch en la misma memoria. En este caso, las dos instrucciones deben utilizar los mismos recursos y debería darsele prioridad a la primera instrucción dejando a la segunda en espera.
	
	Para resolver esto, debería agregarse harwdare (como buffers, caches, etc).
	\item \textbf{De datos (Data Hazards):} Hay una instrucción en el pipeline que depende de los resultados de otra instrucción (tambien en el pipeline) y debe esperar a que ésta se complete para poder terminar. Esto puede bloquear el pipe durante varios ciclos de reloj ya que se debe procesar completamente la primer instrucción. 
	
	Para minimizar el impacto de este obstáculo, por lo general, se agrega extra hardware que permite conseguir el valor deseado apenas sea calculado (cuando termina la etapa de ejecución) directamente de los componentes internos para no tener que esperar a que sea guardado en memoria (esta técnica se llama \textbf{forwarding} o \textbf{bypassing}).
	\item \textbf{De control (Control or Branch Hazards):} La instrucción que se fetcheó no es la que debe ejecutarse en este ciclo de reloj. Esto sucede cuando una de las instrucciones del pipe es de control. Cuando es fetcheada, el pipeline no puede saber cual es la próxima instrucción que debe ser ejecutada ya que esto depende del resultado de la instrucción actual.
	
	Una posible solución es parar apenas se haga el fetch del condicional y esperar hasta obtener sus resultado. Otra, es realizar predicciones (\textbf{branch prediction}) y ejecutar las instrucciones con más probabilidad de ser ejecutadas. En este último caso, el pipeline procede sin demoras si la predicción fue correcta, sino debe eliminar las instrucciones procesadas y volver a comenzar desde el lugar correcto.
\end{itemize}

\newpage
\section{Branch Prediction}\label{sec::branchPrediction}

\paragraph{Branching:} Cuando se fetchea una instrucción condicional, se genera una ramificación en el código. Hay dos caminos posibles por el que puede seguir la ejecución y el camino que se debe ejecutar lo decide el resultado de la instrucción fetcheada. 

\paragraph{Untaken Branch:} Se dice cuando el camino tomado es el que ejecuta la instrucción que le sigue al condicional dentro del programa.

\paragraph{Taken Branch:} Cuando se ejecuta la instrucción que se encuentra en la dirección del salto de la instrucción condicional.

\paragraph{Branch penalty:} Cuando se fetchea la instruccción incorrecta, debe descartarse todo lo que estaba pre procesado hasta ese momento. El pipeline se vacía y deben transcurrir $n-1$ ciclos de clock hasta el próximo resultado ($n = $ cantidad de etapas del pipeline).

\subsection{Predicciones estáticas}
\begin{itemize}
	\item \textbf{Assume Branch Not Taken:}  Se fetchea la siguiente instrucción secuencial del programa. Si el salto no se realiza, entonces la ejecución del pipe continúa sin problemas. Si el salto se realiza, se aplica el branch penalty.
	\item \textbf{Assume Branch Taken}: Análogo al anterior, pero esta vez, se fetchea la instrucción en la dirección de memoria apuntada por el salto.
	\item \textbf{Predict by Opcode}: Se asume que el salto va a ser tomado o no dependiendo de la instrucción a ser ejecutada.
\end{itemize}

Este tipo de predicciones funcionan bien para piplines simples. Sin embargo, la penalidad de descartar instrucciones y el tiempo que se tarda en restaurar el sistema aumenta acorde a la complejidad del mismo. Las predicciones dinámicas intentan disminuir este problema manteniendo estadísticas de uso y modificando la decisiones tomadas a medida que se realiza la ejecución.

\subsection{Predicciones dinámicas}

\begin{itemize}
	\item\textbf{Branch Prediction Buffer}: Se mantiene una tabla indexada por la dirección de memoria de la instrucción del salto y 2 bits que indican si los últimos dos saltos hacia esa instrucción fueron tomados o no.
	
	\begin{figure}[ht]
		\centering
		\includegraphics[width=0.7\linewidth]{imagenes/2bit-buffer-prediction}
		\caption{Maquinas de estados de predicción para un buffer de 2 bits}
		\label{fig:2bit-buffer-prediction}
	\end{figure}

	Las primeras implementaciones de esta técnica hacían uso de un único bit que se remplazaba cada vez que la predicción fallaba. En ciertos casos, la eficiencia de este método no era satisfactoria llegando a fallar completamente en otros(por ejemplo, si los saltos termina formando una secuencia intercalada de Taken y Not taken).
	
	\item \textbf{Branch Target Buffer (BTB)}: Es una caché en la que se almacena, en cada entrada, la dirección de la instrucción de control y la dirección de la instrucción que fue ejecutada despues de resolver el branching.
	
	Cuando se fetchea una instrucción de control, se accede a esta caché usando el valor del program counter.
	\begin{itemize}
		\item Si el valor no está en el buffer, entonces se asume \textit{taken}.
		\begin{itemize}
			\item Si el resultado es \textit{Non-Taken} se acepta el delay en el pipeline y no se almacena nada en el BTB.
			\item Si el resultado es \textit{taken}, se ingresa el valor al BTB.
		\end{itemize}
		\item Si el valor se encuentra en el BTB, se aplica el campo de dirección de target almacenado.
		\begin{itemize}
			\item si el resultado es \textit{taken} no hay penalidad. No se guarda en el BTB ningún nuevo valor ya que el que está almacenado es el que nos sirve.
			\item Si el resultado es \textit{Non-taken}, guarda el nuevo valor en el BTB, luego de la penalidad correspondiente en el pipeline. 
		\end{itemize}
	\end{itemize}
\end{itemize}

\newpage
\section{Procesadores Superscalares}
Son procesadores que explotan la ejecución paralela de instrucciones para mejorar el rendimiento. Agrega más pipelines a la ejecución, lo que permite incrementar aún más la cantidad de instrucciones procesadas por ciclo de reloj.

Sin embargo, los obstáculos estructurales quedan más expuestos. Cada etapa, ademas de lidiar con los data hazzards de su propio pipeline, debe lidiar tambien con las mismas etapas del otro pipeline. 

Este tipo de procesadores analizan los binarios secuenciales de los programas y lo paralelizan eliminando secuencialidad innecesaria. Por esta razón, los programas binarios deben ser vistos más como una especificación de lo que debe hacerse y no como lo que realmente sucede.

Más precisamente, un procesador superescalar implementa:

\begin{enumerate}
	\item Estrategias de fetch que permiten fetchear múltiples instrucciones mediante la predicción 1de resultados y saltos.
	\item Métodos para determinar dependencias de registros y mecanismos para comunicar esos valores cuando sea necesario durante la ejecución.
	\item Métodos para iniciar, o resolver, múltiples instrucciones en paralelo.
	\item Recursos para la ejecución en paralelo de varias instrucciones, incluyendo multiples unidades funcionales de pipelines y jerarquías de memorias capaces de atender simultáneamente múltiples referencias a memoria.
	\item Métodos para manejar datos a través de instrucciones de lectura/escritura e interfaces de memoria que tengán en cuenta el comportamiento dinámico (y muchas veces impredecibles) de las jerarquías diseñadas.
	\item Métodos para actualizar los estados del proceso en el orden correcto (para mantener la apariencia de la ejecución en orden secuencial).
\end{enumerate}

\subsection{Dependencias de instrucciones}\label{sec:instructionLevelParalelism:dependenciaDeInstrucciones}
Por lo general, se interpreta el binario de un programa como un conjunto de bloques compuestos de instrucciones contiguas. Una vez que un bloque es fetcheado, se sabe que todas sus instrucciones van a ser ejecutadas eventualmente. Cuando esto suceda, diremos que el bloque es iniciado en una \textbf{ventana de ejecución}.

Una vez que las instrucciones entran en esta ventana, son ejecutadas en paralelo teniendo en cuenta sus depencencias. Éstas, pueden ser de control o datos. Las de control son generadas por condicionales y pueden ser resueltas/optimizadas con predicciones (Seccion \ref{sec::branchPrediction}). 

Las \textbf{dependencias de datos} se da entre instrucciones que referencian el mismo espacio de memoria. En estos casos, si las instrucciones no se ejecutan en el orden correcto, puede haber errores en las operaciones. Pueden ser Verdaderas o Artificales.
\begin{itemize}
	\item \textbf{Dependencias verdaderas:} Cuando una instrucción debe leer un valor que todavía no fue generado por una instrucción previa (\textbf{Read after write hazard}).
	\item \textbf{Dependecias artificiales:} Resultan de instrucciones que deben escribir un nuevo valor en una posición de memoria pero debe esperar a que las instrucciones previas utilizen el valor actual (\textbf{Write after Read hazzard}) o cuando varias instrucciones deben escribir la misma posición de memoria (\textbf{Write after Write hazzard}).
	
	Estas dependencias son producidas por código no optimizado, por escasez de registros disponibles, por el deseo de economizar el uso de la memoría principal o por ciclos donde una instrucción puede colisionar consigo misma.
	
\end{itemize}


\subsection{Fetching and Decode}\label{ilp::superscalars::fetchDecode}
En los procesadores superescalares, una caché de instrucciones es usada para reducir la latencia y incrementar el ancho de banda del fetching. Esta caché está organizada en bloques o lineas que contienen varias instrucciones consecutivas y almacena el tipo de cada una de ellas (si es de control, de operación, de lectura de memoria, etc). 

El program counter se utiliza para determinar la posición de una instrucción en la caché.

\begin{itemize}
	\item Si se produce un hit, se fetchea el bloque de instrucciones y se suma al program counter el tamaño del bloque fetcheado.
	\item Si hay un miss, el caché, pide la instrucción buscada en memoria y la copia.
	\item Se identifica el tipo de cada instrucción. Si alguna es de control, se realizan las predicciones usando alguno de los métodos mencionado en en la sección \ref{sec::branchPrediction}.
	\item Las instrucciones son decodificadas en  \textbf{tuplas de ejecución} que contienen la operación a ser ejecutada, las identidad de los elementos donde se encuentran los parámetros de entrada y donde deben guardase los resultados. 
	
	En el programa estático, las instrucciones utilizan los registros \textbf{lógicos} (los de la arquitectura). Por esta razón, cuando las instrucciones son decodificadas, cada uno de ellos es mapeado (o renombrado) a un registro físico y las dependencias artificiales son resueltas indicando a las instrucciones involucradas que usen distintos registros físicos (\textbf{Algoritmo de tomasulo}, sección \ref{ilp::tomasulo}).

	\item Una vez que todas las instrucciones asociadas a un registro físico son completadas (modifican el estado visible), el registro es liberado para que pueda ser usado por otro bloque de instrucciones. 		
\end{itemize}


\subsubsection{Out of Order Execution (Tomasulo)}\label{ilp::tomasulo}
El algoritmo de Tomasulo, crea un scheduling dinamico que permite ejecutar instrucciones fuera de orden y habilita el uso de múltiples unidades de ejecución. Para lograr esto necesitamos:
\begin{itemize}
	\item Mantener un ``link'' entre el productor de un dato con su(s) consumidor(es).
	\item Mantener las instrucciones en espera hasta que estén listas para ejecución.
	\item Las instrucciones deben saber cuando sus operandos estan listos (``Ready'').
	\item Despachar (``disparar'') la instrucción a su Unidad Funcional ni bien todos sus operandos estén listos.
\end{itemize}

\paragraph{Register Renaming:} El primer ítem se logra asignando a cada registro un alias. Para eliminar los WAR y WAW hazards, se utiliza una tabla llamada \textbf{Register Alias Table} que asocia un tag con cada operando de una operación.

\paragraph{Register Station (RS):} Una vez realizado el renombre se le asigna, a la instrucción, un RS. Un RS es un subsistema de hardware compuesto de bancos de registros internos que se encarga de mantener las instrucciones en espera hasta que esté lista para ser ejecutada. Éste chequea constantemente por la disponibilidad de los operandos de la instrucción. Para cada uno de ellos cuyo valor no esté disponible, el RS, guarda el \textit{tag} que se le asignó en el paso anterior.

Cada vez que una unidad de ejecución pone disponible un operando, transmite el \textbf{tag} asociado al mismo junto con su valor a todas las RS. Cuando una instrucción tiene todos sus operandos disponible, el RS espera a que la unidad funcional asociada a ella esté libre y luego la despacha.

\paragraph{Common Data Bus}: Es un datapath que cruza la salida de las unidades funcionales y atraviesa las RS, los Floating Point Buffers, los Floating Points Registers y el Floating Point Operations Stack. Se usa para realizar el broadcast de los resultados al finalizar a cada operación.

\paragraph{Pseudocodigo:}
Cada Registro contiene un tag que indica el último escritor en el registro.

\begin{algorithmic}
	\If{RS tiene recursos disponibles antes del renaming}
	\State{Se inserta en la RS la instrucción y los opernados renombrados (valor fuente / tag)}
	\State{Se renombra si y solo si la RS tiene recursos disponibles.}
	\Else
	\State{stall}
	\EndIf
	\While{esté en la RS, cada instrucción debe}
	\State{Mirar el tráfico por el \textbf{Common Data Bus (CDB)} en busca de \textbf{tags} que corespondan a sus Operandos fuente.}
	\State{Cuando se detecta un \textbf{tag}, se graba el valor de la fuente y se mantiene en RS.}
	\State{Cuando ambos operandos están disponibles, la instrucciónn se marca \textbf{Ready} para ser despachada}
	\If{Unidad Funcional disponible}
	\State{Se depscacha la instrucción a esa unidad funcional}
	\EndIf
	\If{Finalizada la ejecución de la instrucción}
	\State{La unidad funcional pone el valor correspondiente al \textbf{tag} en el \textbf{CBD} (\textbf{tag broadcast})}
	\If{El archivo de registros está conectado al CDB}
	\If{tag del Archivo de Registro $==$ tag broadcast}
	\State{Registro = valor broadcast}
	\State{bit de validez = '1'}
	\State{Recupera \textbf{tag} renombrado}
	\State{No queda copia válida del tag en el sistema}
	\EndIf
	\EndIf
	\EndIf
	\EndWhile
\end{algorithmic}


\subsection{Ejecución de instrucciones}
\subsubsection{Interrupciones}
Existen dos tipos de interrupciones:
\begin{itemize}
	\item \textbf{Excepciones} (o trampas): Se generan cuando se produce un error durante la ejecución o el fetching de ciertas instrucciones. Por ejemplo, opcodes ilegales, errores numéricos o page faults.
	\item \textbf{Interrupciones externas}: Son causadas por instrucciones específicas y dispositivos externos qué están ejecutando algún proceso. Por ejemplo las interrupciones generadas por los dispositivos de entrada/salida (mouse, teclados, pantallas), timers, etc.
\end{itemize}

Cuando ocurre una interrupción, el software o el hardware (o una combinación de ambos) guardan el estado del proceso interrumpido. Este estado consiste, generalmente, del program counter, los registros y la memoria.

\paragraph{Interrupción precisa:} Una interrupción es precisa si el estado guardado es consistente con la arquitectura secuencial del modelo. Osea se debe cumplir que:
\begin{enumerate}
	\item Todas las instrucciones previas a la indicada por el program counter deben haber sido ejecutadas y deben haber modificado el estado del proceso correctamente.
	\item Ninguna de las instrucciones siguientes a la indicada por el program counter debe haber sido ejecutadas ni deben haber modificado el estado del proceso.
	\item Si la interrupción es causada por una excepción en una instrucción del programa, entonces el program counter guardado debe apuntar a la instrucción interrumpida.
\end{enumerate}

Si el estado guardado es inconsistente con el modelo de arquitectura secuencial y no satisface estas condiciones, entonces la interrupción es \textbf{imprecisa}. 

Hay varias formas de evitar las interrupciones imprecisas:

\subsubsection{In-Order Instruction Completion}
Las instrucciones modifican el estado del proceso solo cuando se sabe que todas las instrucciones previas están libres de excepciones. Para asegurar esto se utiliza un registro llamado ``result shift register'' que contiene una tabla de $n$  entradas ($n$ la longitud del pipeline más largo). Una instrucción que toma $i$ ciclos de reloj reserva la $i$-ésima entrada de la tabla y en cada ciclo se la desplaza una posición hacia abajo.
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\linewidth]{imagenes/shift-register}
	\caption{Result Shift Register}
	\label{fig:shiftregister}
\end{figure}

Si la $i$-esima entrada contiene información valida, entonces se pausa la instrucción. En el proximo ciclo, se rechequea si la entrada está libre y se realiza el movimiento.

Para evitar que una instrucción más corta se complete antes que otra de mayor longitud (cuando este es el orden deseado) se rellenan con información invalida todas las entradas anteriores que no fueron reservadas. De esta forma, la nueva instrucción es pausada hasta el próximo ciclo de reloj.

\subsubsection{Reorder-Buffer}
La principal desventaja del método anterior, es que instrucciones rápidas serán retenidas a pesar de no tener dependencias.

Para resolver este problema, se agrega un ``reorder buffer`` que indica el orden en el que las instrucciones deben modificar el estado visible. Entonces, se permite que las instrucciones terminen en cualquier orden pero se les asigna un tag que cumple con el objetivo mencionado. Cuando son completadas, se las almacena en la entrada indicada (por el tag) del buffer.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{imagenes/reorder-buffer}
	\caption{Reorder buffer}
	\label{fig:reorder-buffer}
\end{figure}

El buffer contiene un puntero a la entrada que corresponde a la próxima instrucción a ser ejecutada. Cuando dicha entrada contenga información valida, se  chequea si hubo excepciones. Sí no las hubo, se modifican los registros/memoria necesarios y se mueve el puntero a la próxima entrada. Sino, se emite la excepción y se invalidan todas las entradas posteriores.

En algunos casos, la dependencia entre instrucciones requiere el uso de valores que todavía no fueron escritos en los registros. Para conseguirlos es necesario acceder al buffer de reordenamiento, lo que agrega latencia y complejidad a la ejecución.

\paragraph{History Buffer:} Una solución posible es dejar que las instrucciones modifiquen los registros cuando son completadas, pero que se guarden los valores previos en un buffer que permita recuperar estos datos en el caso de una excepción.

En este caso, si ocurre una excepción, se debe recorrer el history buffer para poder recuperar un estado preciso.

\paragraph{Future File:} Otra solución es mantener dos arhicvos de registros. Uno que se actualice en el orden especificado por el programa (\textbf{Architectural File}). Y otro que se actualiza apenas se completa una instrucción (\textbf{Future File}).

Si ocurre una excepción simplemente se debe mostear el Arquitecture File para conseguir un estado preciso. Sin embargo, hay que copiarlo al future file para restaurar el estado del sistema.

\paragraph{Checkpointing:} Cuando se predice el resultado de una instruccion de control, se guarda el estado del future file (\textbf{checkpoint}). Si al ejecutarla, la predicción resulta ser errónea1 entonces solo hay que copiar la información del checkpoint en el future file para recuperar el estado del sistema y comenzar de nuevo en la instrucción correcta.  

\newpage
\part{Subsistema de memoria}
\section{El sistema de memoria}
\subsection{Principio de vencidad o localidad}
\paragraph{Temporal Locality:} Una dirección de memoria que está siendo accedida actualmente tiene muy alta probabilidad de seguir siendo accedida en el futuro inmediato.

\paragraph{Spatial Locality}: Si se está accediendo a una dirección determinada de memoria actualmente, la probabilidad de que está dirección y sus adyacentes sean accedidas en el futuro inmediato es muy alta.

La localidad de los programas surge naturalmente de su estructura. La mayoría contiene loops, las instrucciones y los datos usados en ellos será accedido repetidamente lo que genera localidad temporal. Además, como las instrucciones del programa son accedidas secuencialmente se tiene localidad espacial.

\subsection{Jerarquías de memoria}
Para aprovechar estos principios se implementan la memoria de una computadora como una \textbf{jerarquía de memoria}. Está consiste en múltiples niveles de memoria con diferentes tamaños y velocidades. Mientas más rápida sean, más caras por bit son. El objetivo es presentar al usuario con tanta memoria como sea posible con la tecnología más barata proveyendo, al mismo tiempo, la velocidad de acceso provista por las memorias más rápidas.

En la actualidad, hay tres tipos de tecnologías usadas para construir las distintas jerarquías. La memoría principal es implementada con DRAM (Dynamic Random Acces Memory) mientras que los niveles más cercanos al procesador (caché) usan SRAM (Static Random Acces Memory)  [Ver sección \ref{sec::Memoria::Tipos}].

\begin{figure}[ht]
	\centering
	\includegraphics[width=1\textwidth]{imagenes/memory-heriarchy}
	\caption{Memory Hierarchy}
	\label{fig:memory-hierarchy}
\end{figure}

Sin importar el tamaño de la jerarquía, los datos son copiados solo entre dos niveles adyacentes. El nivel más alto - el más cercano al procesador - es más pequeño y rapido ya que usa tecnología más cara que el nivel más bajo.

\paragraph{Bloque:} La mínima unidad de información que puede estar presente en la jerarquía de dos niveles (la que está compuesta por las dos memorias que intercambian información).

\paragraph{Hit:} Se produce cuando la información pedida por el procesador se encuentra en algún bloque de la memoria que se está utilizando.

\paragraph{Miss:} Cuando la información debe ser buscada en el nivel inferior de la jerarquía.

\paragraph{Hit Rate:} La fracción de accesos a memoria encontrados en cada de la jerarquía. A menudo es usado como medida de rendimiento de la misma.

\paragraph{Miss Rate: } 1 - hit rate (para cada nivel)

\paragraph{Hit time:} El tiempo que se tarda en acceder a un bloque en cada nivel de la jerarquía.

\paragraph{Miss penalty:} El tiempo requerido para fetchear un bloque desde el nivel inferior de la jerarquía (incluyendo tiempo de acceso, transmisión y copiado del bloque).

\newpage
\section{Tipos de memoria}\label{sec::Memoria::Tipos}
Hay dos tipos de memoria:


\begin{itemize}
	\item \textbf{No volátiles:} Son memorias capaces de retener la información almacenada cuando se les desconecta la alimentación. Son la tercera digievolución de las memorias \textbf{ROM} (Read Only Memory) que debían ser grabadas por el fabricante del chip y no eran modificables.
	
	De ROM pasaron a ser componentes programables que podían ser borrados con luz ultravioleta de una determinada longitud de onda. Y, luego, se convirtieron en las actuales memorias flash que pueden ser grabadas por algoritmos de escritura \textit{on the fly} por el usuario. El ejemplo más habitual son los discos de estado sólido de los equipos portátiles modernos.
	
	Se usan fundamentalmente para almacenar el programa de arranque de cualquier sistema.
	
	\item\textbf{Volatiles:} Conocidas como \textbf{RAM}, se caracterizan por que una vez interrumpida la alimentación eléctrica, la información que almacenaban se pierde.
	
	Estas memorias pueden almacenar mayores cantidades de información y modificarla en tiempo real a gran velocidad a comparación de las No Volátiles.
	
	Se clasifican de acuerdo con la tecnología y su diseño interno en:
	
	\begin{itemize}
		\item \textbf{Dinámicas (DRAM):} Almacenan la información en forma de una carga en un capacitor y la sostiene durante un breve lapso de tiempo con la ayuda de un transistor.
		
		Para guardar la información, se activa el transistor que aplica el voltaje apropiado a la linea del bit. Una vez cargado, el capacitor, se desactiva. 
		
		Después de un tiempo, la carga del capacitor (que se empieza a descargar) llega a un determinado threshold y en necesario volver a activar el transistor para cargar la información otra vez (si había un uno).
		
		Durante una operación de lectura, los capacitores de la celda seleccionada son activados y descargados completamente. En este caso, un amplificador detecta los valores leídos y aplica voltaje a las lineas de bits necesarias para volver a cargar
 		los capacitores necesarios. Esto aumenta el tiempo de acceso a la celda ya que no se puede liberar la operación hasta no haber repuesto el estado de carga de cada uno de ellos.
 		\item \textbf{Estáticas (SRAM):} Almacenan la información en un biestable. Una celda se compone de seis transitores (por lo que tienen menos capacidad por componente que las dinámicas)
 		
 		Tres de los seis transistores están saturados (conducen la máxima corriente posible de forma permanente) y los otros tres están al corte (conducen una corriente prácticamente insignificante pero no nula). Esto genera un mayor consumo de energía por celda.
 		
 		La lectura es directa y no destructiva, lo que se traduce en un tiempo de acceso bajo en comparación con las memorias dinámicas.
	\end{itemize}

\end{itemize}

\newpage
\section{Memoria Caché}
Los niveles de caché son bancos de SRAM de muy alta velocidad, que contienen una copia de los datos e instrucciones que están en memoria principal. Éstos deben ser lo suficientemente grandes para que el procesador resuelva la mayor cantidad posible de búsquedas de código y datos de memoria asegurando una alta performance y lo suficientemente pequeñas para no afectar el consumo ni el costo del sistema.

Para implementar estas memorias, se debe agregar un controlador que se encarga de mantener la caché actualizada y de indicarle que datos debe mandar al procesador.

\subsection{Operción de lectura}
\begin{itemize}
	\item El procesador inicial un ciclo de lectura de memoria, envía la dirección necesaria al controlador de caché.
	\item El controlador, busca la dirección en el directorio de la caché.
	\begin{itemize}
		\item Si hay un \textbf{hit}, busca el ítem en la memoria caché y lo envía al procesador.
		\item Si hay un \textbf{miss},  busca el ítem en el sistema de memoria y lo copia en la caché. Actualiza el directorio de la misma y después manda la data al procesador.
	\end{itemize}
\end{itemize}

\begin{figure}[ht]
	\centering
	\includegraphics[width=1\textwidth]{imagenes/cache}
	\caption{Operación de lectura}
	\label{fig:reading-operation}
\end{figure}

\subsection{Organización de la caché}

\paragraph{Línea:} Elemento mínimo de palabra de datos dentro del caché. Corresponde a un múltiplo del tamaño de la palabra de datos de memoria. Esto nos permite copiar, en memoria, el ítem requerido y aquellos que lo rodean (principio de vecindad espacial).

\paragraph{Set:} Un conjunto que contiene $2^{ln}$ lineas de la caché.

\subsubsection{Mapeo directo}
Se divide, a la memoria en $2^j - 1$ paginas. Y cada página en $n$ bloques.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.75\textwidth]{imagenes/cache-mapeo-directo}
	\caption{Mapeo directo}
	\label{fig:direct-mapping}
\end{figure}
\begin{itemize}
	\item Los primeros $j$ bits de la dirección identifican la página a la que pertenece la línea.
	\item Los siguientes $n$, el set que les corresponde en la caché.
	\item Los $ln$ bits identifican la línea que le corresponde dentro del set
	\item y los últimos $b$ bits indican el índice de la palabra buscada dentro de la línea.
\end{itemize}

Para  buscar una dirección de memoria, en este tipo de caché:

\begin{itemize}
	\item Se busca el set en el que se encontraría esa dirección.
	\item Si el set tiene información válida, se corrobora que sea de la página de memoria a la que pertenece esa dirección.
	\begin{itemize}
		\item Si la página es correcta, entonces se accede al set en busca de la línea correspondiente y se vuelve el dato pédido.
		\item Sino, se trae de memoria un bloque de tamaño del set y remplaza el set actual por ese.
	\end{itemize}
\end{itemize}

\subsection{Asociativa de dos vías}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.75\textwidth]{imagenes/cache-asociativo-vias}
	\caption{Asociativo por 2 vías}
`	\label{fig:asociativo-mapping}
\end{figure}

Cuando se trae un bloque de memoria, éste puede ser copiado en cualquiera de las dos mitades mientras sea dentro del set correspondiente. Se utiliza un algoritmo (generalmente least recently used - LRU)) para decidir cual mitad remplazar con la nueva información.

\subsection{Operación de escritura (Coherencia de caché)}

Cuando modificamos un valor, sería ideal que el cambio se vea reflejado tanto en la caché como en memoria principal. Si esto no pasa, entonces diremos que las memorias son inconsistentes/incoherentes.

Dependiendo del sistema (si hay una sola CPU o más de una) se utilizan distintas \textbf{políticas de escritura}. Decidir cuál de ellas usar constituye una de las decisiones más importantes en el diseño del sistema de memoria:

\begin{itemize}
	\item \textbf{Write through:} Cada vez que hay que hay que modificar una dirección, el procesador manda el dato tanto a memoria príncipal como al controlador de la caché y se realiza la escritura en ambas memorias. 
	
	Esto garantiza coherencia entre ambos datos de manera absoluta pero penaliza cada escritura con el tiempo de acceso a DRAM. Osea, que la performance se degrada durante estas operaciones.
	
	\item \textbf{Write through buffered:} El procesador manda la data al controlador de caché que la actualiza y sigue ejecutando instrucciones y usando datos de la misma.
	
	El controlador dispone de un buffer de escritura que va almacenando estas modificaciones mientras se espera qe sean escritas a memoria. Cuando la escritura finaliza, se desencola y sigue con la próxima modificación.
	
 	Si el buffer está lleno cuando el procesador pide una escritura, entonces se debe parar la ejecución y esperar a que tenga una entrada libre. Si bien, la ocurrencia de los stalls es reducida, estos siguen pasando.
 	
 	\item \textbf{Copy back / Write back:} Se modifican solo las líneas de la caché y se marcan como \textbf{dirty} (modificadas). El controlador, escribe el bloque en memoria cuando debe ser remplazada por otro. 
 	
 	Este método mejora el performance cuando el procesador puede generar escrituras tan o más rápido de lo que puede escribir en memoria. Sin embargo, es mucho más difícil de implementar.
\end{itemize}

Si el procesador realiza un miss mientras el controlador caché está accediendo a la DRAM, para actualizar el valor, deberá esperar hasta que se termine la actualización para poder acceder a la misma.

\subsubsection{Coherencia en sistemas multi-procesador (Snoop Bus)}
El bus es un conjunto de cables que conecta varios dispositivos, cada uno de los cuales puede observar cada transacción del bus. Cuando un procesador emite un pedido a su caché, el controlador examina el estado de la misma y realiza las acciones adecuadas para completarlo. Esto puede generar transacciones para acceder a memoria.

En este caso, cada procesador posee una caché de primer nivel y todas ellas son conectadas al subsistema de memoria principal a través un único bus que es compartido. 

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{imagenes/multiprocesor-caches}
	\caption{Bus de memoria compartido}
	\label{fig:busCompartido}
\end{figure}

Cuando un procesador modifica un bloque de memoria presente en su caché, se produce una incoherencia en las caches de los otros procesadores que tenían su propia copia del bloque. Esto sucede porque la misma contiene un valor obsoleto en la dirección de memoria modificada por el primer procesador.

%\paragraph{Coherencia (en multiprocesadores):} Un sistema de memoria es coherente si es posible, para toda dirección, construir un orden serial de todas las operaciones que acceden a ella tal que:
%\begin{itemize}
%	\item Es consistente con los resultados de le ejecución.
%	\item Las operaciones emitidas por un procesador ocurren en el orden que fueron producidas por el mismo.
%	\item El valor devuelto por cada lectura es el valor escrito por la última operación de escritura en ese orden.
%\end{itemize}
%
%Está implícita, en esta definición, la propiedad de \textbf{serialización de escritura}. Ésta nos asegura que todos los procesadores ven todas las escrituras a una dirección en el mismo orden.

%\paragraph{Snooping bus:}
La coherencia se mantiene haciendo que cada controlador de caché espíe el bus y monitoreé las transacciones. El controlador del bus funciona como arbitro para  decidir el orden en el que estas son ejecutadas. Cuando una transacción es enviada por el bus, se envia la dirección a la que se está accediendo y el tipo de operación que se está realizando sobre ella.

Cada controlador de cache, toma la dirección enviada por el bus y chequea si tiene una copia del bloque referenciado por la misma.

\begin{itemize}
	\item \textbf{con write-through:} Todas las escrituras se realizan directamente en la memoria principal. Todas las caches que tengan una copia del bloque modificado invalidan esa entrada. De esta forma, cuando el procesador necesite leer esa dirección, se tendrá que volver a cargar el bloque desde la memoria principal.
	
	El problema con este método es que se accede a memoria por cada operación de almacenamiento.
	
	\item  \textbf{write-back:} Al momento, es el método más utilizado ya que reduce drásticamente estos accesos. Sin embargo, no puede ser usado directamente en estos sistemas, ya que no sería posible identificar donde está el último valor válido de una dirección. Por esta razón, se desarrollaron protocolos de coherencias (el más popular de ellos M.E.S.I) que nos permiten identificar, a través la validez de nuestros datos.
\end{itemize}

\subsubsection{Protocolo MESI}
En este protocolo, un bloque de memoria en la caché puede estar en cuatro estados:

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{imagenes/mesi-state-graph}
	\caption{Diagrama de estados de MESI}
	\label{fig:mesiDiagram}
\end{figure}

\begin{itemize}
	\item \textbf{Modified (M):} Es la única copia valida. La memoria principal está desactualizada y ninguna otra caché tiene una copia valida del mismo bloque.
	\item \textbf{Exclusive (E):} Esa caché es la única que contiene una copia de ese bloque. Además, esa copia, no está modificada.
	\item \textbf{Shared (S):} Está presente en esa caché sin ninguna modificación, la memoria principal está actualizada y puede haber otra caché que también lo tenga.
	\item \textbf{Invalid (I):} No está presente en la caché o contiene información desactualizada.
\end{itemize}

\paragraph{Lectura de un bloque:}
Supongamos que el procesador necesita leer un bloque de memoria, entonces produce un processor read (\textbf{PR}):
\begin{itemize}
	\item Si el bloque esta en caché (\textbf{Modified}, \textbf{Exclusive} o \textbf{Shared}), se resuelve el pedido.
	\item Si el bloque es \textbf{Invalid}, entonces se genera un busRead (\textbf{BR}) y todas las cachés responden con una señal indicando si tienen o no una copia del valor pedido:
	\begin{itemize}
		\item Si no está en ninguna otra caché, entonces se lo carga en modo \textbf{Exclusive} desde memoria principal.
		\item Si lo tiene alguna otra, se copia el bloque en modo \textbf{shared} después de haber tomado las medidas necesarias para mantener la consistencia. Si la otra caché lo tiene:
		\begin{itemize}
			\item En modo \textbf{Exclusive}, se lo pasa a estado \textbf{Shared}.
			\item En modo \textbf{Modified}, se copian a memoria principal los cambios realizados y se lo pasa a \textbf{Shared}
			\item En modo \textbf{Shared}, no se hace nada.
		\end{itemize}
	\end{itemize}
\end{itemize}

\paragraph{Escritura de un bloque:} Ahora, para escribir en un bloque, el procesador primero debe asegurarse que es el único con permisos de escritura, entonces:

\begin{itemize}
	\item Si el dato está en caché en modo \textbf{Modified} o \textbf{Exclusive}, genera un processor write (\textbf{PW}) y se modifica la caché. En el segundo caso, se pasa al estado \textbf{Modified}.
	\item Si está en \textbf{Invalid} ó \textbf{Shared}:
	\begin{itemize}
		\item Genera un Processor Request For Ownership (\textbf{PRFO}). Esto hace que todas las otras cachés invaliden sus copias del bloque.
		\item Luego, genera el proccesor write (\textbf{PW}), realiza las modificaciones y pasa el bloque a \textbf{Modified}
	\end{itemize}
\end{itemize}
\newpage
\part{Procesadores Intel}
\section{The microarhchitecture of the Pentium 4 Processor (Netburst microarchitecture)}

La microarquitectura de este procesador se puede separar en 4 grandes bloques: Front End, Out-of-order engine, Integer and Floating Point Execution Units and the memory subsytem.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.35\textwidth]{imagenes/p4-block-architecture}
	\caption{Diagrama básico}
	\label{fig:p4DiagramaBasico}
\end{figure}
\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{imagenes/p4-architecture}
	\caption{Architectura del procesador}
	\label{fig:p4DiagramaBasico}
\end{figure}

\subsection{Front end}
Se dedica al fetcheo y decodificación de instrucciones y la predicción de saltos. Esta compuesto por:
\subsubsection*{Instruction Translation Lookaside Buffer (TLB)}
Traduce las direcciones lineales del puntero de instrucciones en las direcciones físicas necesarias para acceder a la caché L2. Además, realizar los chequeos de permisos de paginación.

\subsubsection*{Front end Branch Target Buffer}
Fetchea las instrucciones IA-32 que deben ser ejecutadas de la caché L2. El uso de Branch Prediction permite al procesador comenzar a fetchear y ejecutar instrucciones mucho antes de que el outcome de las ramificaciones sea averiguado.
	
Si una rama no se encuentra en el BTB, el hardware de predicciones decide que hacer dependiendo el desplazamiennto de la rama (\textbf{forward} o \textbf{backward}). Backwards branches se asumen Taken y Forward Branches se asumen Not-Taken.
	
\subsubsection*{IA-32 Instruction Decoder}
Recibe las instrucciones IA-32 desde la caché L2 y las decodifica en primitivas (llamadas micro-operaciones). Una vez decodificadas, si la instrucción consiste en menos de 4 micro-operaciones, entonces es almacenada en la \textbf{Trace Caché}. Sino es enviada a la \textbf{microcode ROM}.

\subsubsection*{Trace Caché} 
Es la caché de instrucciones primaria o de Nivel 1 (\textbf{L1}) donde se guardan ya decodificadas. La mayoría de las instrucciones son fetcheadas y ejecutadas desde esta caché. Solo cuando hay un caché miss, la microarquitectura, fetchea y decodifica desde la instrucciones desde la caché \textbf{L2}.
	
Las instrucciones IA-32 tienen un número variable de bits y muchas opciones. La lógica de decodificación necesita tener en cuenta esto y convertirlas en micro-operaciones que la máquina sepa como ejecutar. Esto se hace especialmente difícil cuando se tratan de decodificar varias instrucciones en un único ciclo de reloj de alta frecuencia.
	
Además, cuando la predicción de un branch es errónea, el tiempo de recuperación es mucho más corto si la máquina no tiene que re-decodificar las instrucciones IA-32 necesarias para reanudar la ejecución en el lugar correcto.
	
El cacheo de las micro operaciones permite, a la microarquitectura, evitar el uso decodificador de instrucciones IA-32 durante gran parte de la ejecución.
	
La caché toma las micro-operaciones decodificadas y las ensambla en secuencias ordenadas llamadas trazas (\textbf{traces}). El orden es el mismo en el que ocurrirían si el programa fuese ejecutado secuencialmente. Esto permite que el target de los branch sean incluidos en la misma linea de traza incluso si estan separados por miles de bytes en el programa.

La caché tiene su propio BTB (TBTB) para predecir cual es la próxima micro-operación a ser ejecutada. Esta es más chica que la BTB principal ya que solo debe realizar predicciones sobre el subset de instrucciones presentes en la caaché.
\subsubsection*{Microcode ROM}
Esta ROM es usada para instrucciones IA-32 complejas como mover strings y para manejar faults e interrupciones.

Cuando se encuentra una instrucción IA-32 compleja, la Trace Chache llama a esta ROM que emite las micro-operaciones necesarias para ejecutarla. Cuando termina de hacer esto, el control vuelve al Trace Caché.

Las micro-operaciones que provienen de la caché y la microcode ROM son encoladas en un buffer ordenado, lo que ayuda a suavizar el flujo de micro-operaciones al Out-Of-Order Engine.

\subsection{Out-of-order engine}
Reordena las instrucciones para que sean ejecutadas apenas todos sus operandos estén listos. Y se encarga de presentar el estado correcto al front end cuando es necesario modificarlo.

El procesador inte encontrar tantas instrucciones como sea posible ejecutar cada ciclo de reloj. El motor fuera de orden ejecutará todas aquellas que estén listas para ser ejecutadas incluso si no lo hacen en el orden original del programa.

\subsubsection*{Allocator (Asignador?)}
El allocator se encarga de distribuir los recursos (entradas del archivo de registros, buffers, alus, etc)) del sistema entre las micro operaciones que deben ser ejecutadas. Si un recurso no está disponible para alguna de las micro-operaciones que llegaron desde el front end, entonces se pausa la asignación por un ciclo de reloj. Cuando todos los recursos están disponibles, entonces los asigna a las instrucciones y las despacha al pipeline para que sean ejecutadas.

A todas las instrucciones les asigna una entrada en el reorder buffer, lo que permite mantener un seguimiento del estado de la micro-operación, el registro donde se guardarán los resultados de la micro-operación y una entrada en alguna de las colas que se encuentran en frente del scheduler.

\subsubsection*{Register Renaming}
El proceso de renombre remueve falsos conflictos causados por múltiples instrucciones independientes que necesitan de un mismo registro. Esto se logra creando varias instancias del mismo registro que indiquen quien las escribió por última vez y el valor adecuado. Ver sección \ref{ilp::superscalars::fetchDecode} 

\subsubsection*{Scheduling de micro-operaciones}
Hay dos colas FIFO, una para operaciones de memoria y otra para el resto. Se permite que cada cola sea leída en cualquier orden, lo que permite que la ventana de ejecución fuera de orden sea más grande.

Los schedulers de micro-operaciones determinan cuando una micro-operación está lista para ser ejecutadas y permiten reordenarlas manteniendo las dependencias correctas. 

Para enviar las micro-operaciones a las ALU, existen puertos a los que se conectan múltiples schedulers. Cuando varios de ellos, tiene instrucciones listas para la ejecución, se arbitra el uso de estos puertos. 

\subsection{Integer and Floating Point Excution Units}
Es donde se ejecutan las instrucciones. 

Los registros de enteros y los de puntos flotantes se encuentran separados. Cada archivo de registros, además, está conectado a una red que permite enviar resultados completos que todavía no fueron escritos en los registros a las nuevas micro-operaciones.

\subsubsection*{Low Latency Integer ALU}
Está diseñada para optimizar performance manejando los casos más comunes tan rápido como sea posible. Es capaz de completar operaciones en medio ciclo del reloj principal. Aproximadamente el 60-70\% de las micro-operaciones de enteros pasan por esta ALU. Ejecutandolas a la mitad de la latencia del clock principal ayuda a acelerar la ejecución de la mayoría de los programas.

\subsubsection*{Complex Integer Operation}
Los shift, rotaciones multiplicaciones y divisiones tienen una larga latencia (ocupan varios ciclos de reloj), por esta razón son mandados a otros componentes más complejos. 

\subsubsection*{Low Latency Level 1 (L1) Data Cache}
La latencia de las operaciones de lectura es un aspecto clave en la performance del procesador. Esto es especialmente verdad para los programas del IA-32 porque hay un limitado numero de registros en el ISA. Por esta razón se agrega una pequeña caché de baja latencia que consigue su información desde una caché de segundo nivel. La L2 es más grande, tiene un gran ancho de banda y latencia media.

Con la alta frecuencia y la profundidad del pipeline del pentium 4, el scheduler de micro-operaciones despacha instrucciones más rapido de lo que el load puede ser ejecutado. En la mayoría de los casos, el scheduler asume que hubo en hit en la caché L1. Si la lectura genera un miss entonces habrá, en el pipeline, operaciones dependientes con información errónea que deben volver a ser ejecutadas. Para esto se utiliza un mecanismo conocido como \textbf{replay} que trackea las instrucciones durante su ejecución y, si ocurre el caso mencionado, las detiene e intenta ejecutar nuevamente. 

\subsubsection*{Store-To-Load Forwarding}
Debido a la profunidad del pipeline, una instrucción de almacenamiento puede tomar varios ciclos de reloj para ser retiradas por lo que quizás fueron completadas pero sus resultados todavía no se encuentran en caché. Además de que, por lo general, tienen que esperar a que previos stores que ya fueron terminados terminen de actualizar la caché y commiteen el nuevo estado.
 
A menudo, otras instrucciones deben hacer uso de estos datos y hacerlas esperar hasta que estén en memoria deterioraría el performance del pipeline. Por esta razón, se utiliza un buffer de escrituras pendientes (\textbf{pending store buffer}) que permite que los reasultados que todavía no fueron escritos en cache sean utilizados por las nuevas operaciones. Este proceso es llamado \textbf{store-to-load forwarding}.

Este forwarding es permitido si un load utiliza la misma dirección que un store que halla sido completado y se encuentre en este buffer. Además, el dato pedido debe ser del mismo tamaño o más chico que el store pendiente e iniciar en la misma dirección. Si la información pedida por el load se superpone parcialmente con la información del store o necesita hacer uso de más de dos stores pendientes, entonces el forwarding no se permite y  la instrucción debe conseguir su data desde la caché luego de que las operaciones pendientes sean commiteadas.

\subsubsection*{FP/SSE Execution Units}
Es donde se ejecutan las operacion de punto flotante, MMX, SSE y SSE2.
\subsection{Memory Subsysten}
El sistema de memoria, compuesto por una caché L2 y el bus del sistema.

\subsubsection*{Level 2 Instruction and Data Cache}
Contiene las instrucciones que hacen miss en la Trace Cache y la información que produce un miss en la cache L1. Es una caché asociativa de 8 vías y usa un algoritmo write-back para escribir las modificaciones en memoria.

A esta caché se asocia un prefetcher que monitorea los patrones de acceso a memoria y recuerda la historia de los misses para detectar flujos de datos concurrentes y rellenar la caché con información que podría ser utilizada más adelante por el programa. Además, intenta minimizar el prefetching de datos que podrían causar una sobrecarga de la memoria del sistema y provocar el delay de los accesos que el programa realmente necesita.

\subsubsection*{400Mhz System Bus}
Tiene un ancho de banda de 3.2 Gbytes por segundo, esto permite que las aplicaciones realizen data streaming desde la memoria. Tiene un protocolo que divide las transacciones y un pipeline profundo para permitir que el subsistema de memoria atienda varios accesos/escrituras simultaneamente. 
\newpage
\part*{Bibliografía}
\nocite{*}
\printbibliography[keyword=intro,title={Introduccion}]
\printbibliography[keyword=ilp,title={Instruction Level Paralellism}]
\printbibliography[keyword=memory,title={Jerarquías de memoria}]
\printbibliography[keyword=intel,title={Procesadores Intel}]
\end{document}

